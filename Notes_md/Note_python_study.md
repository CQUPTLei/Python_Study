<center><font size=5 color=green>Python语言基础</font></center>

# 一、语法基础

## 1.1 代码注释

单行注释：井号（#）

```python
# 这是单行注释
```

多行注释：一对3个单引号（‘’‘ ‘’’）或一对3个双引号（“”“ “””）

```python
'''
这是多行注释
这是多行注释
'''
```

## 1.2 缩进

4个空格或一个Tab。代码中不要混用。官方推荐4个空格。

## 1.3 语句断行

一般，一条语句占一行。

语句后面不需要使用封号（与matlab不同，即使加了封号，也会输出）。

长语句使用”\“来换行；

括号里面换行，可以直接回车，一般系统都可以识别。

换行的第二行一般用4个空格，使代码美观、层次清晰。

## 1.4 引号的问题

字符串可用单引号或双引号引起来。

字符串中含有单引号，可在其前加一个“\”；或者在使用双引号引起来的字符串中直接使用单引号。

```python
string = 'I\' a girl.'
string_1 = "I' a girl."
```

# 二、程序结构

# 三、函数

| 最新模型             | 描述                                                                                                                                          | 最大请求         | 训练数据           |
|:-----------------|:--------------------------------------------------------------------------------------------------------------------------------------------|:-------------|:---------------|
| text-davinci-003 | 功能最强大的 GPT-3 模型。可以完成其他模型可以完成的任何任务，通常具有更高的质量、更长的输出和更好的指令遵循。还支持在文本中[插入](https://platform.openai.com/docs/guides/completion/inserting-text)补全。 | 4,000 tokens | Up to Jun 2021 |
| text-curie-001   | Very capable, but faster and lower cost than Davinci.                                                                                       | 2,048 tokens | Up to Oct 2019 |
| text-babbage-001 | 能够执行简单的任务，速度非常快，成本更低。                                                                                                                       | 2,048 tokens | Up to Oct 2019 |
| text-ada-001     | 能够执行非常简单的任务，通常是 GPT-3 系列中最快的型号，而且成本最低。                                                                                                      | 2,048 tokens | Up to Oct 2019 |

| 最新模型             | 描述                                                                                                                | 最大请求               | 训练数据           |
|:-----------------|:------------------------------------------------------------------------------------------------------------------|:-------------------|:---------------|
| code-davinci-002 | 功能最强大的 Codex 型号。特别擅长将自然语言翻译成代码。除了补全代码，还支持在代码中[插入](https://platform.openai.com/docs/guides/code/inserting-code)补全。 | 8,000 tokens       | Up to Jun 2021 |
| code-cushman-001 | 几乎与 Davinci Codex 一样强大，但速度稍快。这种速度优势可能使其成为实时应用程序的首选。                                                               | Up to 2,048 tokens |                |

| 参数                | 类型              | 是否必要     | 默认值       | 解释                                                                                                                                                                                                                                       |
|-------------------|-----------------|----------|-----------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| model             | string          | Required | -         | 要使用的模型的ID。您可以使用列表模型API来查看所有可用的模型，或者查看我们的模型概述来了解它们的描述。                                                                                                                                                                                    |
| prompt            | string or array | Optional | endoftext | 用于生成补全的提示符，编码为字符串、字符串数组、令牌数组或令牌数组。注意endoftext是模型在训练期间看到的文档分隔符，因此如果没有指定提示符，模型将像从新文档的开头生成一样。                                                                                                                                               |
| suffix            | string          | Optional | null      | 在插入文本完成后出现的后缀。                                                                                                                                                                                                                           |
| max_tokens        | integer         | Optional | 16        | 输出生成的最大token数。输入的token数加上max_tokens不能超过模型的context length。大多数模型的context length为2048个tokens(除了最新的模型，它支持4096个)。                                                                                                                             |
| temperature       | number          | Optional | 1         | 使用什么取样温度。更高的值意味着模型将承担更多的风险。对于更有创意的应用程序，可以尝试0.9；对于有明确答案的应用程序，可以尝试0 (argmax抽样)。我们通常建议修改这个或top_p，但不建议同时修改。                                                                                                                                  |
| top_p             | number          | Optional | 1         | temperature采样的替代方法称为核采样，其中模型考虑具有top_p概率质量的标记的结果。所以0.1意味着只考虑包含前10%概率质量的tokens。我们通常建议改变这个或temperature，但不建议两者都改变。                                                                                                                           |
| n                 | integer         | Optional | 1         | 为每个输入生成多少个输出。注意:因为这个参数会生成很多输出，所以它会很快消耗掉你的token配额。请谨慎使用，并确保您对max_tokens有合理的设置，然后停止。                                                                                                                                                       |
| stream            | boolean         | Optional | false     | 是否回流部分进度。如果设置了，tokens将在它们可用时作为仅数据的服务器发送事件发送，stream由“data: [DONE]消息”终止。                                                                                                                                                                   |
| logprobs          | integer         | Optional | null      | 包括logprobs最可能token的对数概率，以及所选token。例如，如果logprobs为5,API将返回5个最可能的token的列表。API将始终返回采样token的logprobb，因此响应中最多可能有logprobs+1个元素。logprobs的最大值为5。如果您需要更多，请通过我们的帮助中心联系我们，并描述您的用例。                                                                   |
| echo              | boolean         | Optional | false     | 除了输出之外，还回显输入                                                                                                                                                                                                                             |
| stop              | string or array | Optional | null      | 最多4个序列，API将停止生成进一步的令牌。返回的文本将不包含停止序列。                                                                                                                                                                                                     |
| presence_penalty  | number          | Optional | 0         | 介于-2.0和2.0之间的数字。正值会根据新token到目前为止是否出现在文本中来惩罚它们，从而增加模型谈论新主题的可能性。请参阅有关频率和存在惩罚的更多信息。                                                                                                                                                         |
| frequency_penalty | number          | Optional | 0         | 介于-2.0和2.0之间的数字。正值会根据新符号在文本中的现有频率来惩罚它们，从而降低模型逐字重复同一行的可能性。请参阅有关频率和存在惩罚的更多信息。                                                                                                                                                              |
| best_of           | integer         | Optional | 1         | 在服务器端生成best_of补全，并返回“best”(每个令牌具有最高日志概率的那个)。结果无法进行流式传输。当与n一起使用时，best_of控制候选补全的数量，n指定返回多少- best_of必须大于n。注意:因为这个参数会生成很多补全，所以它会很快消耗掉你的令牌配额。请谨慎使用，并确保您对max_tokens有合理的设置，然后停止。                                                                |
| logit_bias        | map             | Optional | null      | 修改指定令牌在补全中出现的可能性。接受一个json对象，该对象将标记(由GPT标记器中的标记ID指定)映射到从-100到100的关联偏差值。您可以使用这个标记器工具(适用于GPT-2和GPT-3)将文本转换为标记id。在数学上，偏差被添加到抽样前由模型生成的对数中。每个模型的确切效果会有所不同，但介于-1和1之间的值应该会减少或增加选择的可能性;像-100或100这样的值应该导致相关令牌的禁止或排他选择。例如，您可以传递{“50256”:-100}来防止生成< |
| user              | string          | Optional | -         | 代表终端用户的唯一标识符，可以帮助OpenAI监视和检测滥用。学习更多的知识。                                                                                                                                                                                                  |
